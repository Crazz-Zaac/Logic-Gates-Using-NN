{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdaf954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Possible outputs\n",
    "X = np.matrix('0 0; 1 0; 0 1; 1 1')\n",
    "y = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994bf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(z, 0, z)\n",
    "\n",
    "def relu_prime(z):\n",
    "    \"\"\"First derivative of the ReLU activation function\"\"\"\n",
    "    return 1*(z>0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"First derivative of sigmoid activation function\"\"\"\n",
    "    return np.multiply(sigmoid(z), 1-sigmoid(z))\n",
    "\n",
    "def cost(a, y):\n",
    "    \"\"\"Calculate MSE\"\"\"\n",
    "    return ((a - y) ** 2).mean()\n",
    "\n",
    "def cost_grad(a, y):\n",
    "    \"\"\"First derivate of MSE function\"\"\"\n",
    "    return a - y\n",
    "\n",
    "def weighted_sum(W, a, b):\n",
    "    \"\"\"Compute the weighted average z for all neurons in new layer\"\"\"\n",
    "    return W.dot(a) + b\n",
    "\n",
    "def forward_prop(x, W, b): \n",
    "    \"\"\"Calculate z and a for every neuron using current weights and biases\"\"\"\n",
    "    a = [None] * len(layer_sizes)\n",
    "    z = [None] * len(layer_sizes)\n",
    "    \n",
    "    a[0] = x.T\n",
    "    \n",
    "    for l in range(1, len(a)):\n",
    "        z[l] = weighted_sum(W[l], a[l-1], b[l])\n",
    "        a[l] = sigmoid(z[l])\n",
    "        \n",
    "    return (a, z)\n",
    "\n",
    "def back_prop(a, z, W, y):\n",
    "    \"\"\"Calculate error delta for every neuron\"\"\"\n",
    "    delta = [None] * len(layer_sizes)\n",
    "    end_node = len(a)-1\n",
    "    \n",
    "    delta[end_node] = np.multiply(cost_grad(a[end_node], y), sigmoid_prime(z[end_node]))\n",
    "    \n",
    "    for l in reversed(range(1, end_node)):\n",
    "        delta[l] = np.multiply(W[l+1].T.dot(delta[l+1]), sigmoid_prime(z[l]))\n",
    "    \n",
    "    return delta\n",
    "\n",
    "def calc_gradient(W, b, a, delta, eta):\n",
    "    \"\"\"Update W and b using gradient descent steps based\"\"\"\n",
    "    W_grad = [None] * len(W)\n",
    "    b_grad = [None] * len(b)\n",
    "    \n",
    "    for l in range(1, len(W)):\n",
    "        W_grad[l] = a[l-1].dot(delta[l].T)\n",
    "        b_grad[l] = delta[l]\n",
    "    \n",
    "    return (W_grad, b_grad)\n",
    "\n",
    "def backpropagation_iter(X, y, W, b, eta):\n",
    "    \"\"\"One iteration of the backpropagation algorithm, i.e., forward- and backward propagate and compute gradient\"\"\"\n",
    "    y_pred = [None] * len(y)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # First we propagate forward through the network to obtain activation levels and z.\n",
    "        a, z = forward_prop(X[i, :], W, b)\n",
    "        y_pred[i] = np.max(a[-1])\n",
    "\n",
    "        # Back propagate to obtain delta's.\n",
    "        delta = back_prop(a, z, W, y[i])\n",
    "\n",
    "        # This allows us to compute the gradient for this instance. Add this to all.\n",
    "        W_grad, b_grad = calc_gradient(W, b, a, delta, eta)\n",
    "\n",
    "        if i == 0:\n",
    "            W_grad_sum = W_grad\n",
    "            b_grad_sum = b_grad\n",
    "        else:\n",
    "            for l in range(1, len(W_grad)):\n",
    "                W_grad_sum[l] += W_grad[l]\n",
    "                b_grad_sum[l] += b_grad[l]\n",
    "\n",
    "    # Update weights and bias\n",
    "    for l in range(1, len(W)):\n",
    "        W[l] = W[l] - (eta/n) * W_grad_sum[l]\n",
    "        b[l] = b[l] - (eta/n) * b_grad_sum[l]\n",
    "    \n",
    "    # Show MSE\n",
    "    MSE = cost(y_pred, y)\n",
    "    \n",
    "    return (W, b, y_pred, MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c03ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be4face4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise layer sizes of all layers in the neural network\n",
    "layer_sizes = [X.shape[1], 2, 1]\n",
    "\n",
    "# Initialise weights and activation and weight vectors as None.\n",
    "W = [None] * len(layer_sizes)\n",
    "b = [None] * len(layer_sizes)\n",
    "\n",
    "# Initialise weights randomly\n",
    "for l in range(1, len(layer_sizes)):\n",
    "    W[l] = np.random.random((layer_sizes[l], layer_sizes[l-1]))\n",
    "    b[l] = np.random.random((layer_sizes[l], 1))\n",
    "    \n",
    "# Set number of iterations for backpropagation to work, size, and learning rate\n",
    "n_iter = 10000\n",
    "n = X.shape[0]\n",
    "eta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98dc2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a217aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 0.29154168076363696\n",
      "Iteration 100: 0.25644418613616293\n",
      "Iteration 200: 0.2508820478709627\n",
      "Iteration 300: 0.25015239926399613\n",
      "Iteration 400: 0.2500547498759536\n",
      "Iteration 500: 0.25004375724705\n",
      "Iteration 600: 0.25004585350541564\n",
      "Iteration 700: 0.250097618880977\n",
      "Iteration 800: 0.2500996087852953\n",
      "Iteration 900: 0.25010257503305944\n",
      "Iteration 1000: 0.25010605759556803\n",
      "Iteration 1100: 0.2501098155399221\n",
      "Iteration 1200: 0.25011372106651364\n",
      "Iteration 1300: 0.25011770649182263\n",
      "Iteration 1400: 0.2501217363741961\n",
      "Iteration 1500: 0.2501257926427838\n",
      "Iteration 1600: 0.2501298666234644\n",
      "Iteration 1700: 0.2501339547542252\n",
      "Iteration 1800: 0.2501380562812584\n",
      "Iteration 1900: 0.25014217202060846\n",
      "Iteration 2000: 0.25014630369406526\n",
      "Iteration 2100: 0.2501504535752841\n",
      "Iteration 2200: 0.25015462430421614\n",
      "Iteration 2300: 0.25015881879360063\n",
      "Iteration 2400: 0.25016304018660374\n",
      "Iteration 2500: 0.25016729184370046\n",
      "Iteration 2600: 0.2501715773471257\n",
      "Iteration 2700: 0.25017590051672234\n",
      "Iteration 2800: 0.2501802654339601\n",
      "Iteration 2900: 0.25018467647248227\n",
      "Iteration 3000: 0.2501891383343702\n",
      "Iteration 3100: 0.25019365609175254\n",
      "Iteration 3200: 0.25019823523360535\n",
      "Iteration 3300: 0.25020288171768446\n",
      "Iteration 3400: 0.2502076020275694\n",
      "Iteration 3500: 0.25021240323478966\n",
      "Iteration 3600: 0.250217293065977\n",
      "Iteration 3700: 0.25022227997495144\n",
      "Iteration 3800: 0.2502273732195915\n",
      "Iteration 3900: 0.25023258294328843\n",
      "Iteration 4000: 0.25023792026071956\n",
      "Iteration 4100: 0.2502433973476115\n",
      "Iteration 4200: 0.2502490275340926\n",
      "Iteration 4300: 0.25025482540116495\n",
      "Iteration 4400: 0.25026080687974894\n",
      "Iteration 4500: 0.2502669893516802\n",
      "Iteration 4600: 0.2502733917519592\n",
      "Iteration 4700: 0.2502800346714784\n",
      "Iteration 4800: 0.250286940459373\n",
      "Iteration 4900: 0.25029413332406614\n",
      "Iteration 5000: 0.2503016394320017\n",
      "Iteration 5100: 0.2503094870029883\n",
      "Iteration 5200: 0.25031770640100537\n",
      "Iteration 5300: 0.25032633021925565\n",
      "Iteration 5400: 0.2503353933581879\n",
      "Iteration 5500: 0.25034493309515354\n",
      "Iteration 5600: 0.25035498914431226\n",
      "Iteration 5700: 0.25036560370535244\n",
      "Iteration 5800: 0.2503768214995594\n",
      "Iteration 5900: 0.2503886897917261\n",
      "Iteration 6000: 0.2504012583963873\n",
      "Iteration 6100: 0.25041457966683817\n",
      "Iteration 6200: 0.2504287084653956\n",
      "Iteration 6300: 0.25044370211337014\n",
      "Iteration 6400: 0.25045962031922564\n",
      "Iteration 6500: 0.25047652508343626\n",
      "Iteration 6600: 0.25049448057858514\n",
      "Iteration 6700: 0.2505135530032959\n",
      "Iteration 6800: 0.2505338104086527\n",
      "Iteration 6900: 0.2505553224958278\n",
      "Iteration 7000: 0.2505781603837249\n",
      "Iteration 7100: 0.250602396345533\n",
      "Iteration 7200: 0.2506281035131932\n",
      "Iteration 7300: 0.25065535554889123\n",
      "Iteration 7400: 0.2506842262828164\n",
      "Iteration 7500: 0.2507147893165571\n",
      "Iteration 7600: 0.25074711759165236\n",
      "Iteration 7700: 0.2507812829229664\n",
      "Iteration 7800: 0.2508173554967203\n",
      "Iteration 7900: 0.25085540333318374\n",
      "Iteration 8000: 0.25089549171421366\n",
      "Iteration 8100: 0.25093768257601595\n",
      "Iteration 8200: 0.25098203386770934\n",
      "Iteration 8300: 0.25102859887648443\n",
      "Iteration 8400: 0.2510774255203779\n",
      "Iteration 8500: 0.25112855560992065\n",
      "Iteration 8600: 0.2511820240801837\n",
      "Iteration 8700: 0.2512378581950182\n",
      "Iteration 8800: 0.2512960767255928\n",
      "Iteration 8900: 0.2513566891056619\n",
      "Iteration 9000: 0.2514196945663608\n",
      "Iteration 9100: 0.2514850812537249\n",
      "Iteration 9200: 0.25155282533257417\n",
      "Iteration 9300: 0.2516228900809008\n",
      "Iteration 9400: 0.25169522497944463\n",
      "Iteration 9500: 0.25176976480176083\n",
      "Iteration 9600: 0.2518464287107621\n",
      "Iteration 9700: 0.25192511936848455\n",
      "Iteration 9800: 0.2520057220666643\n",
      "Iteration 9900: 0.2520881038866457\n",
      "Iteration 10000: 0.2521721128981574\n"
     ]
    }
   ],
   "source": [
    "for iter in range(n_iter+1):\n",
    "    W, b, y_pred, MSE = backpropagation_iter(X, y, W, b, eta)\n",
    "    \n",
    "    # Only print every 10 iterations\n",
    "    if iter % 100 == 0:\n",
    "        print('Iteration {0}: {1}'.format(iter, MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb98bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
